{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Reading and writing video files.\n",
    "# Displaying video frames in real-time (cv2.imshow).\n",
    "# Drawing on frames.\n",
    "\n",
    "from tqdm import tqdm # Tracking the progress of video frame processing.\n",
    "from ultralytics import YOLO # a pretrained object detection model\n",
    "\n",
    "import supervision as sv\n",
    "# Handling video frames.\n",
    "# Annotating frames with bounding boxes, labels, and traces.\n",
    "# Defining and working with polygonal zones.\n",
    "\n",
    "from typing import Dict, Iterable, List, Optional, Set\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = sv.ColorPalette.from_hex([\"#E6194B\", \"#3CB44B\", \"#FFE119\", \"#3C76D1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zone Movement Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polygon co-ordinates that mark the roads that lead into roundabout\n",
    "ZONE_IN_POLYGONS = [\n",
    "    np.array([[592, 282], [900, 282], [900, 82], [592, 82]]),\n",
    "    np.array([[950, 860], [1250, 860], [1250, 1060], [950, 1060]]),\n",
    "    np.array([[592, 582], [592, 860], [392, 860], [392, 582]]),\n",
    "    np.array([[1250, 282], [1250, 530], [1450, 530], [1450, 282]]),\n",
    "]\n",
    "\n",
    "# Polygon co-ordinates that mark the roads that lead out of the roundabout\n",
    "ZONE_OUT_POLYGONS = [\n",
    "    np.array([[950, 282], [1250, 282], [1250, 82], [950, 82]]),\n",
    "    np.array([[592, 860], [900, 860], [900, 1060], [592, 1060]]),\n",
    "    np.array([[592, 282], [592, 550], [392, 550], [392, 282]]),\n",
    "    np.array([[1250, 860], [1250, 560], [1450, 560], [1450, 860]]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zone Initialization\n",
    "\n",
    "- Zone initialization happens in the **initiate_polygon_zones** function. \n",
    "- It takes a list of polygon coordinates as input and wraps each polygon in an sv.PolygonZone object.\n",
    "- The PolygonZone class:\n",
    "  -  From the supervision library. Represents a polygon and provides utilities for detecting whether objects are inside it.\n",
    "  - The triggering_anchors parameter specifies which part of the detected object's bounding box is checked for containment. Here, it's set to CENTER, meaning the center point of the bounding box must fall inside the polygon to trigger it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_polygon_zones(\n",
    "    polygons: List[np.ndarray],\n",
    "    triggering_anchors: Iterable[sv.Position] = [sv.Position.CENTER],\n",
    ") -> List[sv.PolygonZone]:\n",
    "    return [\n",
    "        sv.PolygonZone(\n",
    "            polygon=polygon,\n",
    "            triggering_anchors=triggering_anchors,\n",
    "        )\n",
    "        for polygon in polygons\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zone Triggering\n",
    "\n",
    "Zone triggering happens in the **process_frame** function within the **VideoProcessor** class, in the for loop:\n",
    "\n",
    "```python\n",
    "for zone_in, zone_out in zip(self.zones_in, self.zones_out):\n",
    "    detections_in_zone = detections[zone_in.trigger(detections=detections)]\n",
    "    detections_in_zones.append(detections_in_zone)\n",
    "    detections_out_zone = detections[zone_out.trigger(detections=detections)]\n",
    "    detections_out_zones.append(detections_out_zone)\n",
    "```\n",
    "\n",
    "Let's break it done using the first two lines of the loop.\n",
    "\n",
    "- <u>zone_in.trigger(detections=detections)</u>\n",
    "  -  Takes all detections at once and checks if the triggering anchor (e.g., the center of the bounding box) for each detection is inside the polygon (zone_in).\n",
    "  -  Returns a boolean mask, i.e., a NumPy array of True/False values, with eEach value corresponding to whether the center of a detection's bounding box is inside the polygon. E.g for 5 detections, example output would be: [True, False, True, False, False].\n",
    "\n",
    "\n",
    "-  <u>detections[zone_in.trigger(detections=detections)]</u>\n",
    "   -  Returns a subset of the original detections, where the mask is True. From the above example, would return the 1st and 3rd detections.\n",
    "   -  If all are false, it still returns a Detections object, just with empty tracker_id list\n",
    "   -  A sample:\n",
    "\n",
    "```python\n",
    "[\n",
    "    Detections(tracker_id=[101, 102], ...),  # Detections in ZONE_IN_POLYGONS[0]\n",
    "    Detections(tracker_id=[], ...),     # No detections in ZONE_IN_POLYGONS[1]\n",
    "    Detections(tracker_id=[103], ...),    # Detections in ZONE_IN_POLYGONS[2]\n",
    "    Detections(tracker_id=[], ...),     # No detections in ZONE_IN_POLYGONS[3]\n",
    "]\n",
    "```\n",
    "\n",
    "After the above line, the detections_in_zone is appended to a list (detections_in_zones), which collects the detections for all zone_in polygons in the frame. \n",
    "The same happens for zone_out detections. After identifying detections in entry and exit zones, the DetectionsManager class determines if the object transitioned from an entry zone to an exit zone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Mgt.\n",
    "\n",
    "- A supervision.Detections object, a container for detection data, is made up of:\n",
    "  - Bounding box coordinates for the detections that are inside the polygon.\n",
    "  - Tracker IDs for those detections.\n",
    "  - Class IDs (if available).\n",
    "\n",
    "- The DetectionsManager class keeps track of which object (by tracker ID) entered which entry zone, and whether it exited via an exit zone. It also maintains counts of these transitions.\n",
    "\n",
    "**PROPERTIES**\n",
    "-  <u>tracker_id_to_zone_id</u>\n",
    "   -  Dictionary mapping each object's unique tracker ID to the ID of the zone it entered.\n",
    "   -  Example: {101: 0, 102: 2} meaning: Object with tracker ID 101 entered through ZONE_IN_POLYGONS[0], and object with tracker ID 102 entered through ZONE_IN_POLYGONS[2].\n",
    "-  <u>counts</u>\n",
    "   -  Nested dictionary that tracks transitions between zones: {zone_out_id: {zone_in_id: set(tracker_ids)}}.\n",
    "   -  Example: {0: {2: {101, 105}}, 1: {3: {102}}} meaning: Two objects (IDs 101 and 105) moved from ZONE_IN_POLYGONS[2] to ZONE_OUT_POLYGONS[0], and one object (ID 102) moved from ZONE_IN_POLYGONS[3] to ZONE_OUT_POLYGONS[1].\n",
    "  \n",
    "- Using setDefault simplifies the process of ensuring that nested keys exist before modifying the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionsManager:\n",
    "    def __init__(self) -> None:\n",
    "        self.tracker_id_to_zone_id: Dict[int, int] = {}\n",
    "        self.counts: Dict[int, Dict[int, Set[int]]] = {}\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        detections_all: sv.Detections,\n",
    "        detections_in_zones: List[sv.Detections],\n",
    "        detections_out_zones: List[sv.Detections],\n",
    "    ) -> sv.Detections:\n",
    "        for zone_in_id, detections_in_zone in enumerate(detections_in_zones):\n",
    "            for tracker_id in detections_in_zone.tracker_id:\n",
    "                self.tracker_id_to_zone_id.setdefault(tracker_id, zone_in_id)\n",
    "\n",
    "        for zone_out_id, detections_out_zone in enumerate(detections_out_zones):\n",
    "            for tracker_id in detections_out_zone.tracker_id:\n",
    "                if tracker_id in self.tracker_id_to_zone_id:\n",
    "                    zone_in_id = self.tracker_id_to_zone_id[tracker_id]\n",
    "                    self.counts.setdefault(zone_out_id, {})\n",
    "                    self.counts[zone_out_id].setdefault(zone_in_id, set())\n",
    "                    self.counts[zone_out_id][zone_in_id].add(tracker_id)\n",
    "                    \n",
    "        if len(detections_all) > 0:\n",
    "            detections_all.class_id = np.vectorize(\n",
    "                lambda x: self.tracker_id_to_zone_id.get(x, -1)\n",
    "            )(detections_all.tracker_id)\n",
    "        else:\n",
    "            detections_all.class_id = np.array([], dtype=int)\n",
    "        return detections_all[detections_all.class_id != -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The final part of the update function above ensures that only detections with valid tracker_id mappings (i.e., those in the zones) are kept for further processing. It uses np.vectorize to map tracker_id to its associated zone_in_id:\n",
    "  - If the tracker_id exists in tracker_id_to_zone_id, it assigns the corresponding zone_in_id.\n",
    "  - Otherwise, it assigns -1 (invalid ID).\n",
    "  - Finally filters out detections with class_id = -1 (those that did not pass through any ZONE_IN_POLYGONS).\n",
    "- The final output is a list of filtered sv.Detections object containing only valid detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the earlier sections we covered the **process_frame** function that serves as the starting point of video processing.\n",
    "- In **process_frame**, we deal with both object and zone detections. In this section we cover the remiander of process_frame i.e YOLO for object detection & ByteTrack for object tracking.\n",
    "\n",
    "### YOLO\n",
    "\n",
    "**results = self.model(frame, verbose=False, conf=self.conf_threshold, iou=self.iou_threshold)[0]**\n",
    "<br>\n",
    "\n",
    "**detections = sv.Detections.from_ultralytics(results)**\n",
    "\n",
    "- YOLO processes the current frame and outputs a list of detected objects (bounding boxes, confidence scores, etc.).\n",
    "- sv.Detections.from_ultralytics(results) converts the YOLO results into a supervision.Detections object.\n",
    "\n",
    "**detections.class_id = np.zeros(len(detections))**\n",
    "- Initializes the class_id field for the detections.\n",
    "\n",
    "## ByteTrack\n",
    "\n",
    "**detections = self.tracker.update_with_detections(detections)**\n",
    "- The detections for the current frame are passed to ByteTrack for tracking.\n",
    "- ByteTrack updates its internal state and assigns tracker_ids to the detections.\n",
    "- How ByteTrack Matches Detections Across Frames\n",
    "  - Matching with High-Confidence Detections\n",
    "    - ByteTrack uses a high-confidence threshold to create \"active tracks.\" For example, detections with confidence scores above a certain threshold are considered reliable and used to update existing tracks.\n",
    "  - Matching with Low-Confidence Detections\n",
    "    - If no high-confidence detections match a tracked object, ByteTrack uses low-confidence detections to fill gaps. This helps prevent objects from \"disappearing\" momentarily due to occlusions or other issues.\n",
    "  - Kalman Filter\n",
    "    - ByteTrack uses a Kalman filter to predict the position of tracked objects in the next frame, improving its ability to match objects between frames.\n",
    "  - Hungarian Algorithm\n",
    "    - The Hungarian algorithm is used to assign detections to existing tracks based on a cost function, which evaluates how well a detection matches a track. The cost function considers:\n",
    "      - IoU (Intersection over Union) between bounding boxes.\n",
    "      - Distance between predicted and detected positions.\n",
    "  - New Tracks\n",
    "    - If a detection cannot be matched to any existing track, a new track is created with a new tracker_id.\n",
    "\n",
    "\n",
    "- After processing the frame, the derrived detections are passed to **annotate_frame** for annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Annotation\n",
    "- For every frame and list of filtered detections received from **process_frame**, we do 5 key annotations:\n",
    "  - Zone Annotation\n",
    "    - Draws the zone polygons on the frame with their corresponding colours.  \n",
    "  - Trace Annotator\n",
    "    - Shows where each object has been (movement history).\n",
    "    - It uses the historical positions of objects (tracked by their tracker_id) to draw a line or \"trace\" showing the path each object has taken across frames.\n",
    "  - Box Annotator\n",
    "    - Highlights the object’s current position with a bounding box.\n",
    "  - Label Annotator\n",
    "    - Displays custom_text (in this case the tracker_id) for each object, for  identification.\n",
    "  - Zone Statistics\n",
    "    - Shows the counts for how many objects transitioned from each entry zone to each exit zone, based on DetectionManager's counts property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Processing\n",
    "- The **sv.get_video_frames_generator** function creates a generator that yields individual frames from the input video. Total Frames=Video Duration (seconds)×FPS.\n",
    "- The method supports two modes based on whether an output video path (self.target_video_path) is specified.\n",
    "  - Save Processed Video to File: Handles writing processed frames to a video file\n",
    "  - Display Processed Video in Real-Time: The processed frame is displayed in a window using OpenCV's cv2.imshow\n",
    "  \n",
    "- tqdm provides user feedback on processing progress, especially helpful for long videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_weights_path: str,\n",
    "        source_video_path: str,\n",
    "        target_video_path: Optional[str] = None,\n",
    "        confidence_threshold: float = 0.3,\n",
    "        iou_threshold: float = 0.7,\n",
    "    ) -> None:\n",
    "        self.conf_threshold = confidence_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.source_video_path = source_video_path\n",
    "        self.target_video_path = target_video_path\n",
    "\n",
    "        self.model = YOLO(source_weights_path)\n",
    "        self.tracker = sv.ByteTrack()\n",
    "\n",
    "        self.video_info = sv.VideoInfo.from_video_path(source_video_path)\n",
    "        self.zones_in = initiate_polygon_zones(ZONE_IN_POLYGONS, [sv.Position.CENTER])\n",
    "        self.zones_out = initiate_polygon_zones(ZONE_OUT_POLYGONS, [sv.Position.CENTER])\n",
    "\n",
    "        self.box_annotator = sv.BoxAnnotator(color=COLORS)\n",
    "        self.label_annotator = sv.LabelAnnotator(\n",
    "            color=COLORS, text_color=sv.Color.BLACK\n",
    "        )\n",
    "        self.trace_annotator = sv.TraceAnnotator(\n",
    "            color=COLORS, position=sv.Position.CENTER, trace_length=100, thickness=2\n",
    "        )\n",
    "        self.detections_manager = DetectionsManager()\n",
    "        \n",
    "\n",
    "    def process_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        results = self.model(\n",
    "            frame, verbose=False, conf=self.conf_threshold, iou=self.iou_threshold\n",
    "        )[0]\n",
    "        detections = sv.Detections.from_ultralytics(results)\n",
    "        detections.class_id = np.zeros(len(detections))\n",
    "        detections = self.tracker.update_with_detections(detections)\n",
    "\n",
    "        detections_in_zones = []\n",
    "        detections_out_zones = []\n",
    "\n",
    "        for zone_in, zone_out in zip(self.zones_in, self.zones_out):\n",
    "            detections_in_zone = detections[zone_in.trigger(detections=detections)]\n",
    "            detections_in_zones.append(detections_in_zone)\n",
    "            detections_out_zone = detections[zone_out.trigger(detections=detections)]\n",
    "            detections_out_zones.append(detections_out_zone)\n",
    "\n",
    "        detections = self.detections_manager.update(\n",
    "            detections, detections_in_zones, detections_out_zones\n",
    "        )\n",
    "        return self.annotate_frame(frame, detections)\n",
    "    \n",
    "\n",
    "    def annotate_frame(\n",
    "        self, frame: np.ndarray, detections: sv.Detections\n",
    "    ) -> np.ndarray:\n",
    "        annotated_frame = frame.copy()\n",
    "        for i, (zone_in, zone_out) in enumerate(zip(self.zones_in, self.zones_out)):\n",
    "            annotated_frame = sv.draw_polygon(\n",
    "                annotated_frame, zone_in.polygon, COLORS.colors[i]\n",
    "            )\n",
    "            annotated_frame = sv.draw_polygon(\n",
    "                annotated_frame, zone_out.polygon, COLORS.colors[i]\n",
    "            )\n",
    "\n",
    "        labels = [f\"#{tracker_id}\" for tracker_id in detections.tracker_id]\n",
    "        annotated_frame = self.trace_annotator.annotate(annotated_frame, detections)\n",
    "        annotated_frame = self.box_annotator.annotate(annotated_frame, detections)\n",
    "        annotated_frame = self.label_annotator.annotate(\n",
    "            annotated_frame, detections, labels\n",
    "        )\n",
    "\n",
    "        for zone_out_id, zone_out in enumerate(self.zones_out):\n",
    "            zone_center = sv.get_polygon_center(polygon=zone_out.polygon)\n",
    "            if zone_out_id in self.detections_manager.counts:\n",
    "                counts = self.detections_manager.counts[zone_out_id]\n",
    "                for i, zone_in_id in enumerate(counts):\n",
    "                    count = len(self.detections_manager.counts[zone_out_id][zone_in_id])\n",
    "                    text_anchor = sv.Point(x=zone_center.x, y=zone_center.y + 40 * i)\n",
    "                    annotated_frame = sv.draw_text(\n",
    "                        scene=annotated_frame,\n",
    "                        text=str(count),\n",
    "                        text_anchor=text_anchor,\n",
    "                        background_color=COLORS.colors[zone_in_id],\n",
    "                    )\n",
    "\n",
    "        return annotated_frame\n",
    "\n",
    "\n",
    "    def process_video(self):\n",
    "        frame_generator = sv.get_video_frames_generator(\n",
    "            source_path=self.source_video_path\n",
    "        )\n",
    "\n",
    "        if self.target_video_path:\n",
    "            with sv.VideoSink(self.target_video_path, self.video_info) as sink:\n",
    "                for frame in tqdm(frame_generator, total=self.video_info.total_frames):\n",
    "                    annotated_frame = self.process_frame(frame)\n",
    "                    sink.write_frame(annotated_frame)\n",
    "        else:\n",
    "            for frame in tqdm(frame_generator, total=self.video_info.total_frames):\n",
    "                annotated_frame = self.process_frame(frame)\n",
    "                cv2.imshow(\"Processed Video\", annotated_frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "\n",
    "source_weights_path = \"data/traffic_analysis.pt\"                        # Path to the source weights file\n",
    "source_video_path = \"data/traffic_analysis.mov\"                         # Path to the source video file\n",
    "target_video_path = \"output/sample1_result.mp4\"          # Path to the target video file (output)\n",
    "confidence_threshold = 0.3                                              # Confidence threshold for the model\n",
    "iou_threshold= 0.5                                                      # IOU threshold for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 806/806 [03:27<00:00,  3.88it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = VideoProcessor(\n",
    "        source_weights_path=source_weights_path,\n",
    "        source_video_path=source_video_path,\n",
    "        target_video_path=target_video_path,\n",
    "        confidence_threshold=confidence_threshold,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )\n",
    "\n",
    "processor.process_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
